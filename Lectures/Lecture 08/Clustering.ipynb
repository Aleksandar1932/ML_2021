{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.spatial import distance\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "import matplotlib.cm as cm\n",
    "from scipy import linalg\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "colors = ['navy', 'turquoise', 'darkorange']\n",
    "def make_ellipses(gmm, ax):\n",
    "    for n, color in enumerate(colors):\n",
    "        if gmm.covariance_type == 'full':\n",
    "            covariances = gmm.covariances_[n][:2, :2]\n",
    "        elif gmm.covariance_type == 'tied':\n",
    "            covariances = gmm.covariances_[:2, :2]\n",
    "        elif gmm.covariance_type == 'diag':\n",
    "            covariances = np.diag(gmm.covariances_[n][:2])\n",
    "        elif gmm.covariance_type == 'spherical':\n",
    "            covariances = np.eye(gmm.means_.shape[1]) * gmm.covariances_[n]\n",
    "        v, w = np.linalg.eigh(covariances)\n",
    "        u = w[0] / np.linalg.norm(w[0])\n",
    "        angle = np.arctan2(u[1], u[0])\n",
    "        angle = 180 * angle / np.pi  # convert to degrees\n",
    "        v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "        ell = mpl.patches.Ellipse(gmm.means_[n, :2], v[0], v[1],\n",
    "                                  180 + angle, color=color)\n",
    "        ell.set_clip_box(ax.bbox)\n",
    "        ell.set_alpha(0.5)\n",
    "        ax.add_artist(ell)\n",
    "        ax.set_aspect('equal', 'datalim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and visualize the first 2 features of the Iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "skf = StratifiedKFold(n_splits=4)\n",
    "train_index, test_index = next(iter(skf.split(iris.data, iris.target)))\n",
    "\n",
    "X = iris.data\n",
    "X_train = iris.data[train_index]\n",
    "y_train = iris.target[train_index]\n",
    "X_test = iris.data[test_index]\n",
    "y_test = iris.target[test_index]\n",
    "\n",
    "n_classes = len(np.unique(y_train))\n",
    "plt.figure(figsize=(8,8))\n",
    "for n, color in enumerate(colors):\n",
    "    data = iris.data[iris.target == n]\n",
    "    plt.scatter(data[:, 0], data[:, 1], s=15.5, color=color,\n",
    "                label=iris.target_names[n])\n",
    "for n, color in enumerate(colors):\n",
    "    data = X_test[y_test == n]\n",
    "    plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and visualize the results of 4 Gaussian Mixture Models (GMM) with different covariance matrices (spherical, diagonal, tied, full) on the Iris dataset  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Try GMMs using different types of covariances.\n",
    "estimators = {cov_type: GaussianMixture(n_components=n_classes,\n",
    "              covariance_type=cov_type, max_iter=20, random_state=0)\n",
    "              for cov_type in ['spherical', 'diag', 'tied', 'full']}\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "plt.figure(figsize=(4 * n_estimators // 2, 8))\n",
    "plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n",
    "                    left=.01, right=.99)\n",
    "\n",
    "for index, (name, estimator) in enumerate(estimators.items()):\n",
    "    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n",
    "                                    for i in range(n_classes)])\n",
    "    \n",
    "    estimator.fit(X_train)\n",
    "\n",
    "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
    "    make_ellipses(estimator, h)\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        data = iris.data[iris.target == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n",
    "                    label=iris.target_names[n])\n",
    "    # Plot the test data with crosses\n",
    "    for n, color in enumerate(colors):\n",
    "        data = X_test[y_test == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n",
    "\n",
    "\n",
    "    plt.text(0.05, 0.8, 'BIC score: %.1f' % estimator.bic(X_test),\n",
    "             transform=h.transAxes)\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "\n",
    "plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the probabilities of belonging to cluster 2 for all points of the Iris dataset using the 4 trained Gaussian Mixture Models (GMM) from above. (For each point, soft clustering techniques such as GMM return a probability of belonging to each cluster, with brighter colors representing higher probabilities and darker colors representing lower probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "estimators = {cov_type: GaussianMixture(n_components=n_classes,\n",
    "              covariance_type=cov_type, max_iter=30, random_state=0)\n",
    "              for cov_type in ['spherical', 'diag', 'tied', 'full']}\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "plt.figure(figsize=(4 * n_estimators // 2, 8))\n",
    "plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n",
    "                    left=.01, right=.99)\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(estimators.items()):\n",
    "    estimator.means_init = np.array([X_train[y_train == i].mean(axis=0)\n",
    "                                    for i in range(n_classes)])\n",
    "    \n",
    "    estimator.fit(X_train)\n",
    "\n",
    "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
    "    make_ellipses(estimator, h)\n",
    "\n",
    "    class2_probabilities = estimator.predict_proba(X_train)[:,2]\n",
    "    data = X_train\n",
    "    plt.scatter(data[:,0], data[:,1], marker='o', norm = mpl.colors.Normalize(0,1), c=class2_probabilities, cmap=\"inferno\")\n",
    " \n",
    "    class2_probabilities = estimator.predict_proba(X_test)[:,2]\n",
    "    data = X_test\n",
    "    plt.scatter(data[:,0], data[:,1], marker='o', norm = mpl.colors.Normalize(0,1), c=class2_probabilities, cmap=\"inferno\")\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "    plt.text(0.05, 0.8, 'BIC score: %.1f' % estimator.bic(X_test),\n",
    "             transform=h.transAxes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We plot the iterations of the EM algorithm with random initialization of the parameters on the Iris dataset for different covariances matrices as shown above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the iterations for a GMM with a spherical covariance matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = [1,2,3,5, 7, 10]\n",
    "       \n",
    "estimators = {cov_type: GaussianMixture(n_components=n_classes,\n",
    "              covariance_type=\"spherical\", init_params=\"random\", n_init=1, max_iter=cov_type, random_state=100)\n",
    "              for cov_type in iterations}\n",
    "\n",
    "n_estimators = len(estimators)\n",
    "plt.figure(figsize=(3 * n_estimators // 2, 8))\n",
    "plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n",
    "                    left=.01, right=.99)\n",
    "\n",
    "for index, (name, estimator) in enumerate(estimators.items()):\n",
    "    estimator.fit(X_train)\n",
    "\n",
    "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
    "    make_ellipses(estimator, h)\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        data = iris.data[iris.target == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n",
    "                    label=iris.target_names[n])\n",
    "    for n, color in enumerate(colors):\n",
    "        data = X_test[y_test == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "\n",
    "plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the iterations for a GMM with a diagonal covariance matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "iterations = [1,2,3,7, 10, 20]\n",
    "       \n",
    "estimators = {cov_type: GaussianMixture(n_components=n_classes,\n",
    "              covariance_type=\"diag\", init_params=\"random\", n_init=1, max_iter=cov_type, random_state=100)\n",
    "              for cov_type in iterations}\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "plt.figure(figsize=(3 * n_estimators // 2, 8))\n",
    "plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n",
    "                    left=.01, right=.99)\n",
    "\n",
    "\n",
    "for index, (name, estimator) in enumerate(estimators.items()):\n",
    "    estimator.fit(X_train)\n",
    "\n",
    "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
    "    make_ellipses(estimator, h)\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        data = iris.data[iris.target == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n",
    "                    label=iris.target_names[n])\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        data = X_test[y_test == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "\n",
    "plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of the iterations for a GMM with a full covariance matric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "iterations = [1,2,3,7, 15, 30]\n",
    "       \n",
    "estimators = {cov_type: GaussianMixture(n_components=n_classes,\n",
    "              covariance_type=\"full\", init_params=\"random\", n_init=1, max_iter=cov_type, random_state=100)\n",
    "              for cov_type in iterations}\n",
    "n_estimators = len(estimators)\n",
    "\n",
    "plt.figure(figsize=(3 * n_estimators // 2, 8))\n",
    "plt.subplots_adjust(bottom=.01, top=0.95, hspace=.15, wspace=.05,\n",
    "                    left=.01, right=.99)\n",
    "\n",
    "for index, (name, estimator) in enumerate(estimators.items()):\n",
    "    estimator.fit(X_train)\n",
    "\n",
    "    h = plt.subplot(2, n_estimators // 2, index + 1)\n",
    "    make_ellipses(estimator, h)\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        data = iris.data[iris.target == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], s=0.8, color=color,\n",
    "                    label=iris.target_names[n])\n",
    "\n",
    "    for n, color in enumerate(colors):\n",
    "        data = X_test[y_test == n]\n",
    "        plt.scatter(data[:, 0], data[:, 1], marker='x', color=color)\n",
    "\n",
    "    plt.xticks(())\n",
    "    plt.yticks(())\n",
    "    plt.title(name)\n",
    "\n",
    "plt.legend(scatterpoints=1, loc='lower right', prop=dict(size=12))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We plot the iterations of the k-means for different random initializations of the parameters on a simulated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_samples = 150\n",
    "random_state = 180\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "iterations = [1,2,3,10]\n",
    "\n",
    "fig2 = plt.figure(figsize=(9, 9))\n",
    "plt.title(\"K-means iterations\")\n",
    "for i in range(len(iterations)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    estimator = KMeans(init='random', n_clusters=3, random_state=170, n_init=1, max_iter=iterations[i])\n",
    "    y_pred = estimator.fit_predict(X) \n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "    plt.scatter(estimator.cluster_centers_[:,0],estimator.cluster_centers_[:,1], s=50, marker='x', color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bad initialization leads to bad final clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 150\n",
    "random_state = 180\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "iterations = [1,2,3,10]\n",
    "\n",
    "fig2 = plt.figure(figsize=(9, 9))\n",
    "plt.title(\"K-means iterations\")\n",
    "for i in range(len(iterations)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    estimator = KMeans(init='random', n_clusters=3, random_state=15, n_init=1, max_iter=iterations[i])\n",
    "    y_pred = estimator.fit_predict(X) \n",
    "\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "    plt.scatter(estimator.cluster_centers_[:,0],estimator.cluster_centers_[:,1], s=50, marker='x', color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the k-means++ algorithm for initialization can severely help this problem as well as lower the number of initializations needed (In the example below, the algorithm has converged from the start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 150\n",
    "random_state = 180\n",
    "X, y = make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "\n",
    "iterations = [1,2,3,10]\n",
    "\n",
    "fig2 = plt.figure(figsize=(9, 9))\n",
    "plt.title(\"K-means iterations\")\n",
    "for i in range(len(iterations)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    estimator = KMeans(init='k-means++', n_clusters=3, random_state=170, n_init=1, max_iter=iterations[i])\n",
    "    y_pred = estimator.fit_predict(X) \n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y_pred)\n",
    "    plt.scatter(estimator.cluster_centers_[:,0],estimator.cluster_centers_[:,1], s=50, marker='x', color=\"black\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For probabilistic models (such as GMM) we can calculate the BIC score for each model. The one with the lowest score should be the best. In the example below, we can see that the lowest BIC score is for a GMM model with a full covariance matrix (since the clusters are obviously with different covariances) and the number of clusters K=2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Number of samples per component\n",
    "n_samples = 500\n",
    "\n",
    "# Generate random sample, two components\n",
    "np.random.seed(0)\n",
    "C = np.array([[0., -0.1], [1.7, .4]])\n",
    "X = np.r_[np.dot(np.random.randn(n_samples, 2), C),\n",
    "          .7 * np.random.randn(n_samples, 2) + np.array([-6, 3])]\n",
    "\n",
    "plt.figure(figsize=(9, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], .8)\n",
    "plt.show(\"The data\")\n",
    "plt.show()\n",
    "\n",
    "lowest_bic = np.infty\n",
    "bic = []\n",
    "n_components_range = range(1, 7)\n",
    "cv_types = ['spherical', 'tied', 'diag', 'full']\n",
    "for cv_type in cv_types:\n",
    "    for n_components in n_components_range:\n",
    "        # Fit a Gaussian mixture with EM\n",
    "        gmm = GaussianMixture(n_components=n_components,\n",
    "                                      covariance_type=cv_type)\n",
    "        gmm.fit(X)\n",
    "        bic.append(gmm.bic(X))\n",
    "        if bic[-1] < lowest_bic:\n",
    "            lowest_bic = bic[-1]\n",
    "            best_gmm = gmm\n",
    "\n",
    "bic = np.array(bic)\n",
    "color_iter = itertools.cycle(['navy', 'turquoise', 'cornflowerblue',\n",
    "                              'darkorange'])\n",
    "clf = best_gmm\n",
    "bars = []\n",
    "\n",
    "# Plot the BIC scores\n",
    "plt.figure(figsize=(9, 10))\n",
    "spl = plt.subplot(3, 1, 1)\n",
    "for i, (cv_type, color) in enumerate(zip(cv_types, color_iter)):\n",
    "    xpos = np.array(n_components_range) + .2 * (i - 2)\n",
    "    bars.append(plt.bar(xpos, bic[i * len(n_components_range):\n",
    "                                  (i + 1) * len(n_components_range)],\n",
    "                        width=.2, color=color))\n",
    "plt.xticks(n_components_range)\n",
    "plt.ylim([bic.min() * 1.01 - .01 * bic.max(), bic.max()])\n",
    "plt.title('BIC score per model')\n",
    "xpos = np.mod(bic.argmin(), len(n_components_range)) + .65 +\\\n",
    "    .2 * np.floor(bic.argmin() / len(n_components_range))\n",
    "plt.text(xpos, bic.min() * 0.97 + .03 * bic.max(), '*', fontsize=14)\n",
    "spl.set_xlabel('Number of components')\n",
    "spl.legend([b[0] for b in bars], cv_types)\n",
    "\n",
    "# Plot the winner\n",
    "splot = plt.subplot(3, 1, 2)\n",
    "Y_ = clf.predict(X)\n",
    "for i, (mean, cov, color) in enumerate(zip(clf.means_, clf.covariances_,\n",
    "                                           color_iter)):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    if not np.any(Y_ == i):\n",
    "        continue\n",
    "    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "\n",
    "    # Plot an ellipse to show the Gaussian component\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    splot.add_artist(ell)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Best model - GMM: full model, 2 components, BIC='+str(clf.bic(X)))\n",
    "plt.subplots_adjust(hspace=.35, bottom=.02)\n",
    "\n",
    "average_gmm = GaussianMixture(n_components=6,\n",
    "                                      covariance_type=\"full\")\n",
    "average_gmm.fit(X)\n",
    "splot = plt.subplot(3, 1, 3)\n",
    "Y_ = average_gmm.predict(X)\n",
    "for i, (mean, cov, color) in enumerate(zip(average_gmm.means_, average_gmm.covariances_,\n",
    "                                           color_iter)):\n",
    "    v, w = linalg.eigh(cov)\n",
    "    if not np.any(Y_ == i):\n",
    "        continue\n",
    "    plt.scatter(X[Y_ == i, 0], X[Y_ == i, 1], .8, color=color)\n",
    "\n",
    "    # Plot an ellipse to show the Gaussian component\n",
    "    angle = np.arctan2(w[0][1], w[0][0])\n",
    "    angle = 180. * angle / np.pi  # convert to degrees\n",
    "    v = 2. * np.sqrt(2.) * np.sqrt(v)\n",
    "    ell = mpl.patches.Ellipse(mean, v[0], v[1], 180. + angle, color=color)\n",
    "    ell.set_clip_box(splot.bbox)\n",
    "    ell.set_alpha(.5)\n",
    "    splot.add_artist(ell)\n",
    "    \n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.title('Average model - GMM: full model, 6 components, BIC='+str(average_gmm.bic(X)))\n",
    "plt.subplots_adjust(hspace=.35, bottom=.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For non-probabilistic methods (such as K-means) the easiest way to find the best K is to plot the Within Cluster Variation (or Sum of Squared Errors) and the Between Cluster Variation and try to find an elbow in the WCV, with the BCV remaining high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fig1 = plt.figure(figsize=(9, 9))\n",
    "wcv = {}\n",
    "bcv = {}\n",
    "X, y = make_blobs(n_samples=500,\n",
    "                  n_features=2,\n",
    "                  centers=4,\n",
    "                  cluster_std=1,\n",
    "                  center_box=(-10.0, 10.0),\n",
    "                  shuffle=True,\n",
    "                  random_state=1) \n",
    "\n",
    "for k in range(1, 10):\n",
    "    kmeans = KMeans(n_clusters=k,max_iter=1000).fit(X)\n",
    "    wcv[k] = kmeans.inertia_ \n",
    "    centers = kmeans.cluster_centers_\n",
    "    BCV = 0\n",
    "    for i in range(len(centers)):\n",
    "        for j in range(len(centers)):\n",
    "            BCV += distance.euclidean(centers[i], centers[j])**2\n",
    "    if(k==1):\n",
    "        bcv[1] = 0\n",
    "    else:\n",
    "        bcv[k] = BCV/(k*(k-1))*100\n",
    "plt.plot(list(wcv.keys()), list(wcv.values()), label=\"Within Cluster Distance (WCV)\")\n",
    "plt.plot(list(bcv.keys()), list(bcv.values()), label=\"Between Cluster Distance (BCV)\")\n",
    "plt.xlabel(\"Number of clusters K\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
